# Ceph: A Scalable, High-Performance Distributed File System
## 1 简介
类似NFS式的集中式层次结构，服务器导出文件存储层次结构，客户端将其映射到本地。  
基于对象存储，客户机与OSD（对象存储设备）通信执行I/O，与MDS（元数据服务器）通信执行元数据操作。元数据操作没有分发出去。  
通过生成函数代替文件分配表，Ceph解耦了元数据操作和数据操作，由此利用了OSD中的性能来分散压力。
## 2 系统概述
![](img/Ceph_Figure_1.png)
组件：
1. 客户机：提供接近POSIX的文件接口
2. OSD集群：共同存储数据和元数据
3. MDS集群：管理命名空间，同时协调安全性、一致性和连续性

三个基本设计：
1. 数据和元数据解耦
   > 元数据操作（打开、重命名等）由MDS集群管理；客户端与OSD直接交互执行文件I/O。
2. 动态分布式元数据管理
   > 利用基于动态子树分区的数据集群架构在MDS集群之间分配文件系统目录层次结构的责任。
3. 可靠的自主分布式对象存储RADOS
   > Ceph将数据迁移、复制、故障检测和故障恢复的责任委托给OSD集群，能有效利用OSD的性能。
## 3 客户端操作
描述了Ceph客户端如何运行，提供文件系统接口给应用程序，并且如何与OSDs和元数据服务器进行交互。
### 3.1 文件I/O和功能
当进程打开一个文件时，客户端会向MDS集群发送请求。MDS遍历文件系统层次结构，将文件名转换为inode和条带号（stripe number），MDS会下发相应的操作权限capability（客户端读取、缓存读取、写入和缓冲写入）。  
客户端通过CRUSH可以找到每个条带对象对应的OSD，并直接从OSD中读取或写入对象。完成后，客户端需要交还capability，并将新的文件大小告知MDS。
### 3.2 客户端同步
当MDS发现文件被多个客户端并发读写的时候，它就会收回所有客户端的缓冲读写权限（buffer capability），强制client执行同步的读写操作。单个OSD的并发操作的顺序完全由OSD自己决定。  
在科学计算中，并发读写同一个文件是常见的场景，此时就需要放松一致性以获得更好的性能。Ceph提供了POSIX的扩展，比如O_LAZY表示宽松的一致性。另外应用可以自己管理一致性，如保证不同client写文件的不同区域。
### 3.3 命名空间操作
客户端与文件系统的命名空间namespace交互都是同步的。为了降低这些同步namespace操作的延时，Ceph针对常见场景做了优化：比如readdir随后对每个文件调用stat（ls -l），在Ceph中readdir就会加载整个目录的inode，这样随后的stat就会直接返回刚刚加载的内容，牺牲了一致性（假设之间有更新），但保证了性能。
## 4 动态分布式元数据
Ceph中文件和目录的元数据都很小，几乎只包含目录项（文件名）和inode（80B）。与传统文件系统不同的是，Ceph中不需要记录block分配表，而是使用CRUSH定位。这些设计都能缓解MDS的压力，允许它管理海量的文件和目录（大部分缓存在内存中）。
### 4.1 元数据存储
每个MDS维护自己的日志journal，以流式传输streaming的方式存储到OSD的磁盘上，这些日志后续会合并到MDS的长期存储中。  
优势：
1. 以顺序方式将更新流式传输到磁盘，大大减少了重写工作负载。
2. MDS发生故障时，另一个节点可以扫描日志以恢复节点内存中缓存的关键内容。
### 4.2 动态子树分区
MDS集群通过动态子树划分策略来分配缓存元数据。  
每个MDS都使用计数器来衡量目录树中元数据的热度popularity，任何inode操作都会使整个链路上的计数器递增直到根节点，计数器随指数时间衰减。MDS根据该计数器定期比较负载大小，并迁移适当大小的目录子树，使负载均衡。
![](img/Ceph_Figure_2.png)
### 4.3 流量控制
针对许多客户端访问同一目录的情况，Ceph有特殊处理：
1. 读特别热的目录可以复制到多个节点上，以分散负载。
2. 写特别热的目录可以按文件名hash后分散到多个节点上，以在牺牲目录局部性的情况下实现平衡的分布。
## 5 分布式对象存储
解释了Ceph如何通过CRUSH算法将数据分布在OSDs上，以及RADOS如何管理对象复制、集群扩展、故障检测和恢复。
### 5.1 基于CRUSH的数据分发
![](img/Ceph_Figure_3.png)    
步骤：
1. 根据数据对象(object)的对象名，使用哈希函数计算得到PG_ID，将对象映射到放置组（PGs）。
2. 根据PG_ID，使用CRUSH函数计算得到一组OSD，将放置组分配给OSDs。
   > CRUSH全称Controlled Replication Under Scalable Hashing，主要目的是为了定位所存储数据的位置。
### 5.2 复制
数据按放置组进行复制，每个放置组映射到一个有序列表的n个OSD（n路复制）。客户端将所有写操作发送到对象的PG中的第一个非故障OSD（主OSD），该主OSD为对象和PG分配一个新版本号，并将写操作转发给任何其他副本OSD。读操作定向到主OSD。
### 5.3 数据安全
![](img/Ceph_Figure_4.png)
主OSD将更新转发到副本，并在应用于所有OSD的内存缓存之后回复ack，允许客户端上的同步POSIX调用返回。当数据安全地提交到磁盘时，可能会在最终提交时发送ack（也许在许多秒钟后）。默认情况下，客户端还会缓冲写入，直到提交，以避免在放置组中的所有OSD同时断电时发生数据丢失。在这种情况下恢复时，RADOS允许在接受新更新之前回放先前已确认（因此有序）更新的固定时间间隔。
### 5.4 故障检测
OSD会定期发送显式的ping给同PG的其他OSD。不响应的OSD最初被标记为下线，其在每个放置组中的主要责任（更新序列化、复制）临时传递给其放置组中的下一个OSD。如果OSD没有快速恢复，它将被标记为数据分布之外，另一个OSD将加入每个PG以重新复制其内容。与故障的OSD有待处理操作的客户端简单地重新提交到新的主OSD。
### 5.5 恢复和集群更新
OSD维护每个对象的版本号和每个PG的最近更改日志。 
### 5.6 基于EBOFS的对象存储
EBOFS（Extent and B-tree based Object File System）是一个专为Ceph设计的本地对象文件系统，用于管理低级存储。每个Ceph OSD都使用EBOFS管理其本地对象存储。在用户空间完全实现EBOFS并直接与原始块设备交互，使我们能够定义自己的低级对象存储接口和更新语义，将更新序列化（用于同步）与磁盘提交（用于安全性）分离。EBOFS支持原子事务（例如，对多个对象进行写入和属性更新），并在内存缓存更新时返回，同时提供提交的异步通知。
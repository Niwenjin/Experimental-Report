# The Google File System
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung  
Google∗
## 设计目标
1. 组件故障是常态
2. 文件非常庞大
3. 文件更改以添加为主，而不是覆盖
4. 共同设计应用程序和文件系统API以提高灵活性
## 设计概述
### 假设
1. 系统由许多经常失效的廉价组件组成。
2. 系统存储海量的大文件。
3. 工作负载主要由两种读取组成：大型流式读取和小型随机读取。
4. 工作负载还包含大型顺序写入。数据一旦写入就很少修改。
5. 系统必须有效地为同时追加到同一文件的多个客户实现定义明确的语义。
6. 高持续带宽比低延迟更重要。
### 接口
常规操作`create, delete, open, close, read, write`  
快照`Snapshot`以较低的成本创建文件或目录树的副本  
记录追加`record append`允许多个客户端同时将数据追加到同一个文件，同时保证每个客户端追加的原子性。
### 架构
![](img/gfs%20architecture.png)
GFS集群由单个**主服务器**（master）和多个**块服务器**（chunkserver）组成。  
文件被划分为固定大小的块（64MB），每个块都复制到多个块服务器上（默认3个）。  
主服务器在内存中维护文件系统**元数据**（命名空间、访问控制信息、文件到块的映射以及区块的当前位置），并管理系统活动（如垃圾回收、块迁移）。主服务器定期与心跳消息中的每个块服务器通信，向其发出指令并收集其状态。  
客户端与主服务器交互以执行元数据操作，但所有承载数据的通信都直接发送到块服务器，而不是通过主服务器读取和写入文件数据，以免它成为瓶颈。   
客户端和块服务器都不会缓存文件数据（块存储为本地文件，因此Linux的缓冲区缓存已经将频繁访问的数据保存在内存中）。  
**操作日志**记录了元数据重大变更的历史记录，主服务器通过回放操作日志恢复文件系统状态。
### 一致性模型
文件命名空间的变化（如文件创建）是原子性的，由主服务器负责处理，并通过锁和操作日志保证正确性和顺序性。  
“数据突变”后文件区域的状态取决于“突变”的类型、成功还是失败以及是否存在“并发突变”，见表1。
![](img/File%20Region%20State%20After%20Mutation.png)
1. 当一个突变成功而没有并发写器的干扰时，受影响的区域被定义：所有客户端都将看到突变所写的内容。
2. 同时成功的突变的区域未定义但一致：所有客户端都看到相同的数据，但它可能不反映任何一个突变所写的内容。通常，它由来自多个突变的混合片段组成。
3. 失败的突变使区域不一致(因此也未定义)：不同的客户端可能在不同的时间看到不同的数据。
## 系统交互
### 租约和突变顺序
**突变**是一种改变块的内容或元数据的操作，例如写操作或追加操作。每个突变都在所有块的副本上执行。  
**租约**用于维护副本间一致的突变顺序。主服务器将块租约授予其中一个副本（主节点）。主节点为数据块的所有突变选择一个序列顺序。在应用突变时，所有副本都遵循此顺序。  
**租约期限**机制的设计目的是最小化主服务器的管理开销。租约的初始超时为60秒。在块突变的时候，主节点可以请求主服务器获得延期。这些延期请求和授权都承载在主服务器和块服务器之间的常规心跳消息中。有时，主服务器可能会尝试在租约到期之前撤销租约（例如，当主服务器想要禁用正在重命名的文件的突变时）。即使主服务器失去了与主节点的通信，它也可以在旧租约到期后安全地将新租约授予另一个副本。
![](img/write%20control%20&%20data%20flow.png)
1. 客户端询问主服务器主节点（拥有租约）及其他副本的位置。如果没有人拥有租约，主服务器就会将租约授予它选择的副本。
2. 主服务器返回主节点的身份和其他(辅助)副本的位置。接下来只有在主节点无法被触达以及回复不再拥有租约的时候才会再次联系主服务器。
3. 客户端将数据推送到所有副本。
4. 一旦所有副本都确认接收到数据，客户机就向主节点发送写请求。主节点分配连续的序列号给所有它接收到的突变，然后将突变按照自己的序列号设置到它们自己的本地状态。
5. 主节点将写请求转发给所有副节点。每个副节点按照主节点分配的相同序号顺序应用突变。
6. 副节点都回复主服务器，表明它们已完成操作。
7. 主节点回复所有的客户端。在任何副本上遇到的任何错误都会报告给客户端。在出现错误的情况下，它将在步骤(3)到(7)上进行几次尝试，然后从写入的开始重新尝试。
### 数据流
控制流从客户机流向主节点，然后流向所有副节点；数据流沿块服务器链线性推送。将数据流和控制流分离，能够充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，并最大限度地减少延迟。
### 记录追加
GFS提供了一个称为记录追加的原子追加操作。客户端只指定数据，GFS至少自动地将其追加到文件一次，并将该偏移量返回给客户端。若主节点检查追加后块大小超过最大大小（64MB），它将块填充到最大大小，告诉副节点也这样做，并回复客户端，指示应该在下一个块上重试该操作。
### 快照
快照操作几乎是立即生成文件或目录树的副本，同时最大限度地减少正在进行的突变的任何中断。  
当主服务器接收到快照请求时，它首先撤销即将快照的文件中所有未到期的租约（这确保了对这些块的任何后续写操作都需要与主服务器进行交互才能找到租约持有者）。在租约被撤销或过期后，主服务器将操作记录到磁盘。然后，它通过复制源文件或目录树的元数据，将此日志记录应用于其内存状态。
## 主服务器操作
主服务器执行所有命名空间操作。此外，它在整个系统中管理块副本：它做出放置决策，创建新的块和副本，并协调各种系统范围的活动以保持块完全复制，平衡所有块服务器之间的负载，并回收未使用的存储。
### 命名空间管理和锁定
命名空间树中的每个节点(绝对文件名或绝对目录名)都有一个关联的读写锁。每个主服务器操作在运行之前都会获取一组锁。通常，如果它涉及/d1/d2/…/dn/leaf，它将获取目录名/d1， /d1/d2，…，/d1/d2/.../dn的读锁，而无论在全路径/d1/d2/.../dn/leaf上的是读锁还是写锁。
### 副本放置
块副本放置策略有两个目的:最大限度地提高数据可靠性和可用性；最大限度地提高网络带宽利用率。为此，需要将副本分散到不同机架上（将副本分散到机器上只能防止磁盘或机器故障），防止整个机架损坏或脱机，同时可以利用多个机架的总带宽。
### 创建、重新复制、再平衡
创建块副本有三个原因：创建块、重新复制和再平衡。
1. 当主服务器创建一个块时，它来选择位置创建最初的空副本。
2. 一旦某个块可用副本的数量低于用户指定的目标，主服务器就会重新复制这个块：选择优先级最高的块，并通过指示一些块服务器直接从现有的有效副本复制块数据来“克隆”它。  
3. 另外，主服务器会定期重新平衡副本：它检查当前副本分布并移动副本，以获得更好的磁盘空间和负载平衡。
### 垃圾回收
当应用程序删除一个文件时，主服务器会立即记录删除操作，但是不是立即回收资源，而是将文件重命名为隐藏名称。这些隐藏文件将在超过一定期限（通常是三天）后被删除，在此之前，文件仍然可以在新的特殊名称下读取，并且可以通过将其重命名为正常名称来撤销删除。  
在对块命名空间进行类似的常规扫描时，主服务器会识别孤立的块(即无法从任何文件访问的块)，并擦除这些块的元数据。在与主服务器定期交换的心跳消息中，每个块服务器报告它拥有的块的一个子集，主服务器回复已被删除的块元数据，块服务器可以自由地删除这些块的副本。
### 失效副本检测
对于每个块，主服务器维护一个块版本号，以区分最新副本和过期副本。  
每当主服务器授予一个块的新租约时，它都会增加块版本号并通知最新的副本。主服务器和这些副本都在它们的持久状态中记录新的版本号。当块服务器重新启动时，主服务器将检测到这个块服务器：
1. 版本号小于其记录中的版本号，说明这个块服务器有一个过时的副本，主服务器会报告它的块集及其相关的版本号。
2. 版本号大于其记录中的版本号，则主服务器认为它在授予租约时失败，因此采用较高的版本为最新版本。  

主服务器会在其常规垃圾收集中删除陈旧的副本。
## 容错与诊断
### 高可用性
GFS使用**快速恢复**和**复制**来保持整个系统的高可用性。
1. 主服务器和块服务器都能够恢复它们的状态并在几秒钟内启动，无论它们如何终止。
2. 每个块被复制到不同机架上的多个块服务器上。
3. 影子主服务器读取不断增长的操作日志的副本，并将与主服务器相同的更改序列应用于其数据结构。
### 数据完整性
每个块服务器使用校验和来检测存储数据的损坏。一个块被分解成64KB的块，每个都有一个相应的32位校验和。
### 诊断工具
GFS服务器生成诊断日志，这些日志记录了许多重要事件（例如块服务器上升和下降）以及所有RPC请求和应答。
## 性能评估
### 性能指标
使用了三个性能指标来评估GFS的性能：
1. 吞吐量（throughput）：系统在单位时间内处理数据的能力。
2. 延迟（latency）：系统响应请求的时间。
3. 可用性（availability）：系统在故障情况下仍能正常运行的能力。
### 性能测试
使用了两种性能测试来评估 GFS 的性能：
1. 微观测试（micro-benchmarks）：针对 GFS 的特定功能或操作进行的测试，例如读写速度、记录追加、快照等。
2. 实际应用测试（real world applications）：使用 GFS 支持的真实应用程序进行的测试，例如数据分析、数据生成、数据备份等。
### 测试结论
将性能测试的结果与其他分布式文件系统（AFS、xFS等）进行对比，得出结论：
1. GFS 在吞吐量方面表现优异，能够支持大规模的数据处理和分析。
2. GFS 在延迟方面表现一般，但对于大多数应用来说是可以接受的。
3. GFS 在可用性方面表现良好，能够在组件故障的情况下保持数据的完整性和可访问性。